{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "72ac4449",
   "metadata": {},
   "outputs": [],
   "source": [
    "import multiprocessing as mp\n",
    "try:\n",
    "    mp.set_start_method('forkserver')\n",
    "except RuntimeError:\n",
    "    pass  # Already set\n",
    "\n",
    "import os\n",
    "import time\n",
    "import glob\n",
    "import random\n",
    "import _codecs\n",
    "import argparse\n",
    "from pathlib import Path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d7cfefbd",
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'mmcv'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# ─── Local application imports ────────────────────────────────────────────\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mdatasets\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m create_dataset\n\u001b[0;32m----> 3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build_model\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mengine\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m train_one_epoch\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01meval\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mevaluate\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;28meval\u001b[39m\n",
      "File \u001b[0;32m/mnt/stor/gchen-lab/data/Adam/masters-thesis-project/SCSegamba_improved/models/__init__.py:1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mdecoder\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m build\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21mbuild_model\u001b[39m(args):\n\u001b[1;32m      4\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m build(args)\n",
      "File \u001b[0;32m/mnt/stor/gchen-lab/data/Adam/masters-thesis-project/SCSegamba_improved/models/decoder.py:9\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtorch\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m nn\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmcls\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSAVSS_dev\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSAVSS\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mSAVSS\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m SAVSS\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmodels\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mMFS\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m MFS\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mDecoder\u001b[39;00m(nn\u001b[38;5;241m.\u001b[39mModule):\n",
      "File \u001b[0;32m/mnt/stor/gchen-lab/data/Adam/masters-thesis-project/SCSegamba_improved/mmcls/__init__.py:4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Copyright (c) OpenMMLab. All rights reserved.\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mwarnings\u001b[39;00m\n\u001b[0;32m----> 4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmmcv\u001b[39;00m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mpackaging\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m parse\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mversion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m __version__\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'mmcv'"
     ]
    }
   ],
   "source": [
    "\n",
    "# ─── Local application imports ────────────────────────────────────────────\n",
    "from datasets import create_dataset\n",
    "from models import build_model\n",
    "from engine import train_one_epoch\n",
    "from eval.evaluate import eval\n",
    "from util.logger import get_logger\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5391e99",
   "metadata": {},
   "outputs": [],
   "source": [
    "#big chungus imports\n",
    "# ─── Third-party libraries ────────────────────────────────────────────────\n",
    "import torch\n",
    "import torch.backends.cudnn as cudnn\n",
    "from torch.amp import GradScaler, autocast\n",
    "from mmengine.optim.scheduler.lr_scheduler import PolyLR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d49a2a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "DATASET_LIST = [\n",
    "    {\n",
    "        \"name\": \"Crack_Conglomerate\",\n",
    "        \"train\": \"/mnt/stor/ceph/gchen-lab/data/Adam/masters-thesis-project/data/crack_segmentation_unzipped/crack_segmentation/virginia_tech_concrete_crack_congolmeration/Conglomerate Concrete Crack Detection/Conglomerate Concrete Crack Detection/Train\",\n",
    "        \"test\":  \"/mnt/stor/ceph/gchen-lab/data/Adam/masters-thesis-project/data/crack_segmentation_unzipped/crack_segmentation/virginia_tech_concrete_crack_congolmeration/Conglomerate Concrete Crack Detection/Conglomerate Concrete Crack Detection/Test\",\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5a03445",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ─── Third-party libraries ────────────────────────────────────────────────\n",
    "import numpy as np\n",
    "from sklearn.metrics import f1_score, jaccard_score\n",
    "from tqdm import tqdm\n",
    "import cv2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7dbdfc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "add_safe_globals([Namespace, np.core.multiarray.scalar, np.dtype, _codecs.encode]])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b37d0e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_args_parser():\n",
    "    parser = argparse.ArgumentParser('SCSEGAMBA FOR CRACK', add_help=False)\n",
    "    parser.add_argument('--BCELoss_ratio', default=0.87, type=float)\n",
    "    parser.add_argument('--DiceLoss_ratio', default=0.13, type=float)\n",
    "    parser.add_argument('--Norm_Type', default='GN', type=str)\n",
    "    parser.add_argument('--batch_size_train', default=8, type=int)\n",
    "    parser.add_argument('--batch_size_test', default=8, type=int)\n",
    "    parser.add_argument('--lr_scheduler', default='PolyLR', type=str)\n",
    "    parser.add_argument('--lr', default=5e-4, type=float)\n",
    "    parser.add_argument('--min_lr', default=1e-6, type=float)\n",
    "    parser.add_argument('--weight_decay', default=0.01, type=float)\n",
    "    parser.add_argument('--epochs', default=50, type=int)\n",
    "    parser.add_argument('--attention_type', default='gbc_eca', choices=['gbc_eca','eca','sfa','sebica'], help=\"MFS block attention module\", type=str)\n",
    "    parser.add_argument('--start_epoch', default=0, type=int)\n",
    "    parser.add_argument('--lr_drop', default=30, type=int)\n",
    "    parser.add_argument('--sgd', action='store_true')\n",
    "    parser.add_argument('--output_dir', default='./checkpoints/weights', type=str)\n",
    "    parser.add_argument('--device', default='cuda', type=str)\n",
    "    parser.add_argument('--seed', default=42, type=int)\n",
    "    parser.add_argument('--serial_batches', action='store_true')\n",
    "    parser.add_argument('--num_threads', default=8, type=int)\n",
    "    parser.add_argument('--input_size', default=512, type=int)\n",
    "    return parser\n",
    "\n",
    "\n",
    "def export_to_onnx(model, args, dataset_name):\n",
    "    onnx_dir = Path(\"onnx_exports\")\n",
    "    onnx_dir.mkdir(parents=True, exist_ok=True)\n",
    "    model.eval()\n",
    "    dummy_input = torch.randn(1, 1, 512, 512).to(args.device)\n",
    "    torch.onnx.export(\n",
    "        model,\n",
    "        dummy_input,\n",
    "        onnx_dir / f\"{dataset_name}_best.onnx\",\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        export_params=True,\n",
    "        do_constant_folding=True,\n",
    "        opset_version=15,\n",
    "        dynamic_axes={'input': {2: 'height', 3: 'width'}, 'output': {2: 'height', 3: 'width'}}\n",
    "    )\n",
    "    print(f\"[✓] Exported ONNX for {dataset_name}.\")\n",
    "\n",
    "def evaluate_segmentation(preds, labels):\n",
    "    flat_preds = np.concatenate([p.flatten() for p in preds])\n",
    "    flat_labels = np.concatenate([l.flatten() for l in labels])\n",
    "    return {\n",
    "        'F1': f1_score(flat_labels, flat_preds, average='binary'),\n",
    "        'mIoU': jaccard_score(flat_labels, flat_preds, average='binary')\n",
    "    }\n",
    "\n",
    "\n",
    "def train_on_dataset(dataset_cfg, args):\n",
    "    dataset_name = dataset_cfg['name']\n",
    "    onnx_file = Path(\"onnx_exports\") / f\"{dataset_name}_best.onnx\"\n",
    "    if onnx_file.exists():\n",
    "        print(f\"[✓] Skipping {dataset_name} (ONNX already exists)\")\n",
    "        return\n",
    "\n",
    "    # === Resume or create new run directory ===\n",
    "    base_path = Path(args.output_dir) / args.attention_type\n",
    "    results_base = Path(\"results\") / args.attention_type\n",
    "\n",
    "    existing = sorted(glob.glob(str(base_path / f\"*Dataset->{dataset_name}\")), reverse=True)\n",
    "    if existing and getattr(args, 'resume', True):\n",
    "        process_folder = Path(existing[0])\n",
    "        cur_time = process_folder.name.split('_')[0]\n",
    "        print(f\"[✓] Resuming from: {process_folder}\")\n",
    "    else:\n",
    "        cur_time = time.strftime('%Y_%m_%d_%H:%M:%S', time.localtime())\n",
    "        process_folder = base_path / f\"{cur_time}_Dataset->{dataset_name}\"\n",
    "        process_folder.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    results_root = results_base / f\"{cur_time}_Dataset->{dataset_name}\"\n",
    "    results_root.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    checkpoint_file = process_folder / 'checkpoint_best.pth'\n",
    "\n",
    "    # === Loggers ===\n",
    "    log_train = get_logger(str(process_folder), 'train')\n",
    "    log_test  = get_logger(str(process_folder), 'test')\n",
    "    log_eval  = get_logger(str(process_folder), 'eval')\n",
    "    log_train.info(\"Args: \" + str(args))\n",
    "\n",
    "    # === Reproducibility ===\n",
    "    cudnn.benchmark = True\n",
    "    torch.manual_seed(args.seed)\n",
    "    np.random.seed(args.seed)\n",
    "    random.seed(args.seed)\n",
    "    device = torch.device(args.device)\n",
    "\n",
    "    # === Build model and optimizer ===\n",
    "    model, criterion = build_model(args)\n",
    "    model.to(device)\n",
    "    scaler = GradScaler()\n",
    "    optimizer = torch.optim.AdamW(model.parameters(), lr=args.lr, weight_decay=args.weight_decay)\n",
    "    scheduler = PolyLR(optimizer, eta_min=args.min_lr, begin=args.start_epoch, end=args.epochs)\n",
    "\n",
    "    # === Resume from latest epoch if available ===\n",
    "    def extract_epoch(path):\n",
    "        return int(path.stem.split(\"checkpoint_epoch\")[-1])\n",
    "\n",
    "    checkpoints = sorted(\n",
    "        process_folder.glob(\"checkpoint_epoch*.pth\"),\n",
    "        key=extract_epoch,\n",
    "        reverse=True\n",
    "    )\n",
    "\n",
    "    if checkpoints:\n",
    "        latest_ckpt = checkpoints[0]\n",
    "        print(f\"[✓] Resuming from: {latest_ckpt.name}\")\n",
    "        checkpoint = torch.load(latest_ckpt, weights_only=False)\n",
    "\n",
    "        def remap_gbc_to_attention_keys(state_dict):\n",
    "            new_state = {}\n",
    "            for k, v in state_dict.items():\n",
    "                if \"MFS.GBC_C.\" in k:\n",
    "                    new_key = k.replace(\"MFS.GBC_C\", \"MFS.attention\")\n",
    "                    new_state[new_key] = v\n",
    "                else:\n",
    "                    new_state[k] = v\n",
    "            return new_state\n",
    "\n",
    "        checkpoint['model'] = remap_gbc_to_attention_keys(checkpoint['model'])\n",
    "        missing, unexpected = model.load_state_dict(checkpoint['model'], strict=False)\n",
    "        print(\"⚠️ [Patched Load] Missing:\", missing)\n",
    "        print(\"⚠️ [Patched Load] Unexpected:\", unexpected)\n",
    "        optimizer.load_state_dict(checkpoint['optimizer'])\n",
    "        scheduler.load_state_dict(checkpoint['lr_scheduler'])\n",
    "        scaler.load_state_dict(checkpoint['scaler'])\n",
    "        args.start_epoch = checkpoint['epoch'] + 1\n",
    "        best_mIoU = checkpoint.get('best_mIoU', 0)\n",
    "    else:\n",
    "        print(\"[!] No checkpoint found — starting from scratch\")\n",
    "        args.start_epoch = 0\n",
    "        best_mIoU = 0\n",
    "\n",
    "    # === Load training data ===\n",
    "    args.phase = 'train'\n",
    "    args.dataset_path = dataset_cfg['train']\n",
    "    args.batch_size = args.batch_size_train\n",
    "    train_loader = create_dataset(args)\n",
    "\n",
    "    # === Training Loop ===\n",
    "    for epoch in range(args.start_epoch, args.epochs):\n",
    "        log_train.info(f\"Epoch {epoch+1}/{args.epochs}\")\n",
    "        train_one_epoch(model, criterion, train_loader, optimizer, epoch, args, log_train)\n",
    "        scheduler.step()\n",
    "\n",
    "        # Save per-epoch checkpoint\n",
    "        safe_checkpoint = {\n",
    "            'model': model.state_dict(),\n",
    "            'optimizer': optimizer.state_dict(),\n",
    "            'lr_scheduler': scheduler.state_dict(),\n",
    "            'scaler': scaler.state_dict(),\n",
    "            'epoch': epoch,\n",
    "            'best_mIoU': float(best_mIoU),\n",
    "        }\n",
    "\n",
    "        # Optional: only save args if you're confident it's simple\n",
    "        if hasattr(args, '__dict__'):\n",
    "            safe_checkpoint['args'] = {\n",
    "                k: (float(v) if isinstance(v, np.generic) else v)\n",
    "                for k, v in vars(args).items()\n",
    "            }\n",
    "\n",
    "        epoch_ckpt_path = process_folder / f'checkpoint_epoch{epoch}.pth'\n",
    "        torch.save(safe_checkpoint, epoch_ckpt_path)\n",
    "\n",
    "\n",
    "        # === Validation ===\n",
    "        args.phase = 'test'\n",
    "        args.dataset_path = dataset_cfg['test']\n",
    "        args.batch_size = args.batch_size_test\n",
    "        test_loader = create_dataset(args)\n",
    "        model.eval()\n",
    "\n",
    "        all_preds, all_labels = [], []\n",
    "        with torch.no_grad():\n",
    "            for batch in test_loader:\n",
    "                x = batch[\"image\"].to(device, non_blocking=True)\n",
    "                target = batch[\"label\"].to(dtype=torch.int64, device=device)\n",
    "                out = model(x)\n",
    "                out_np = out[0, 0].cpu().numpy()\n",
    "                target_np = target[0, 0].cpu().numpy()\n",
    "\n",
    "                # Save only in the last epoch\n",
    "                if epoch == args.epochs - 1:\n",
    "                    out_img = (255 * (out_np / out_np.max())).astype(np.uint8)\n",
    "                    target_img = (255 * (target_np / target_np.max())).astype(np.uint8) if target_np.max() > 0 else np.zeros_like(target_np, dtype=np.uint8)\n",
    "                    \n",
    "                    name = Path(batch[\"A_paths\"][0]).stem\n",
    "                    final_output_dir = results_root / \"final_outputs\"\n",
    "                    final_output_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "                    cv2.imwrite(str(final_output_dir / f\"{name}_pre.png\"), out_img)\n",
    "                    cv2.imwrite(str(final_output_dir / f\"{name}_lab.png\"), target_img)\n",
    "\n",
    "                # Still needed for in-memory evaluation\n",
    "                all_preds.append((out_np > 0.5).astype(np.uint8))\n",
    "                all_labels.append((target_np > 0.5).astype(np.uint8))\n",
    "\n",
    "        metrics = evaluate_segmentation(all_preds, all_labels)\n",
    "        print(f\"[{dataset_name}] Epoch {epoch}: mIoU = {metrics['mIoU']:.4f}, F1 = {metrics['F1']:.4f}\")\n",
    "        if metrics['mIoU'] > best_mIoU:\n",
    "            best_mIoU = metrics['mIoU']\n",
    "            #torch.save(model.state_dict(), checkpoint_file)\n",
    "            torch.save({\n",
    "                'model': model.state_dict(),\n",
    "                'optimizer': optimizer.state_dict(),\n",
    "                'lr_scheduler': scheduler.state_dict(),\n",
    "                'scaler': scaler.state_dict(),\n",
    "                'epoch': epoch,\n",
    "                'args': args,\n",
    "                'best_mIoU': best_mIoU\n",
    "            }, checkpoint_file)\n",
    "            log_train.info(f\"New best model at epoch {epoch}: mIoU = {best_mIoU:.4f}\")\n",
    "\n",
    "    # === ONNX Export ===\n",
    "    checkpoint = torch.load(checkpoint_file, weights_only=False)\n",
    "    model.load_state_dict(checkpoint[\"model\"])\n",
    "    model.eval()\n",
    "    #dummy_input = torch.randn(1, 3, args.input_size, args.input_size).to(device)\n",
    "    dummy_input = torch.randn(1, 1, args.input_size, args.input_size).to(device)\n",
    "    class ModelWrapper(torch.nn.Module):\n",
    "        def __init__(self, model):\n",
    "            super().__init__()\n",
    "            self.model = model\n",
    "\n",
    "        def forward(self, x):\n",
    "            if x.shape[1] == 1:\n",
    "                x = x.repeat(1, 3, 1, 1)  # (B,1,H,W) → (B,3,H,W)\n",
    "            return self.model(x)\n",
    "\n",
    "    model_to_export = ModelWrapper(model)\n",
    "\n",
    "    torch.onnx.export(\n",
    "        model_to_export,\n",
    "        dummy_input,\n",
    "        onnx_file,\n",
    "        input_names=[\"input\"],\n",
    "        output_names=[\"output\"],\n",
    "        export_params=True,\n",
    "        do_constant_folding=True,\n",
    "        opset_version=15,\n",
    "        dynamic_axes={'input': {2: 'height', 3: 'width'}, 'output': {2: 'height', 3: 'width'}}\n",
    "    )\n",
    "    print(f\"[✓] Finished {dataset_name} — Best mIoU: {best_mIoU:.4f}\")\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    parser = argparse.ArgumentParser('SCSEGAMBA FOR CRACK', parents=[get_args_parser()])\n",
    "    parser.add_argument('--dataset_mode', default='crack', type=str,\n",
    "                        help='Dataset mode selector (required by create_dataset)')\n",
    "    args = parser.parse_args()\n",
    "    for dataset_cfg in DATASET_LIST:\n",
    "        train_on_dataset(dataset_cfg, args)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
